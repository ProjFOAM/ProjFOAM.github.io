<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>FoAM: Foresight-Augmented Multi-Task Imitation Policy for Robotic Manipulation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon_3.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <!-- 添加CSS样式 -->
  <style>
    .video-with-border {
      border: 5px solid #000; /* 5像素的黑色边框 */
      border-radius: 10px; /* 圆角边框，值可以调整 */
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3); /* 添加阴影效果 */
    }
  </style>

  <!-- 样式表：包含目录样式 -->
  <style>
    /* 左侧大纲样式 */
    .toc {
      position: fixed; /* 固定在页面左侧 */
      top: 80px; /* 离顶部的距离 */
      left: 10px; /* 离左侧的距离 */
      width: 250px; /* 导航宽度 */
      background-color: #f4f4f4; /* 背景颜色 */
      border: 1px solid #ddd; /* 边框颜色 */
      padding: 15px; /* 内间距 */
      overflow-y: auto; /* 启用垂直滚动 */
      max-height: 90vh; /* 最大高度，避免内容过多 */
      box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.2); /* 添加阴影效果 */
      border-radius: 5px; /* 圆角 */
    }

    .toc h2 {
      font-size: 18px; /* 标题大小 */
      text-align: center; /* 居中显示 */
      margin-bottom: 10px; /* 标题下方间距 */
    }

    .toc ul {
      list-style-type: none; /* 去除列表符号 */
      padding: 0; /* 去除内边距 */
    }

    .toc li {
      margin: 8px 0; /* 列表项的间距 */
    }

    .toc a {
      text-decoration: none; /* 去除下划线 */
      color: #0073e6; /* 链接颜色 */
      font-size: 14px; /* 链接字体大小 */
    }

    .toc a:hover {
      text-decoration: underline; /* 悬停时显示下划线 */
    }

    body {
      margin-left: 270px; /* 为了避免内容与目录重叠，添加左侧外边距 */
    }
  </style>

</head>
<body>
  <!-- 左侧大纲导航 -->
  <nav id="table-of-contents" class="toc">
    <span>Table of Contents</span>
    <ul id="toc-list">
      <!-- 目录项将在这里动态插入 -->
    </ul>
  </nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">FoAM: Foresight-Augmented Multi-Task Imitation Policy for Robotic Manipulation
          </h1>
          <span style="color: red;">
              The homepage will continuously update!
          </span>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?pli=1&authuser=1&user=r_CvWNYAAAAJ">Litao Liu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a>Wentao Wang</a><sup>2</sup>,</span>
            <span class="author-block">
              <a>Yifan Han</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a>Zhuoli Xie</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a>Pengfei Yi</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a>Junyan Li</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a>Yi Qin</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=-XKzdWQAAAAJ&hl=en&oi=ao">Wenzhao Lian</a><sup>3*</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>CoreNetic.ai,</span>
            <span class="author-block"><sup>2</sup>University of Southern California,</span>
            <span class="author-block"><sup>3</sup>Institute of Automation, Chinese Academy of Sciences</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="./static/pdf_files/ICRA25_2927_MS.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (Under Review)</span>
                </a>
              </span>

              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/LitaoLiu01/FoAM"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a target="_blank" href="https://github.com/LitaoLiu01/FoAM-benchmark"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fas fa-robot"></i>
                    </span>
                    <span>Benchmark</span>
                </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/ProjFOAM/FoAM-dataset"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Dataset</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Concise Project Overview Video</h2>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline controls height="100%" class="video-with-border">
        <source src="https://github.com/ProjFOAM/ProjFOAM.github.io/raw/refs/heads/main/static/videos/AccompanyingVideo.mp4" type="video/mp4">
      </video>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Real-world Inference Videos Showcase</h2>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="hero is-light is-small">
<!--  <h2 class="title is-3">Real-world Inference Videos Showcase</h2>-->
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item PickTheFirstTestTube">
          <div class="video-wrapper">
            <video poster="" id="PickTheFirstTestTube" autoplay controls muted loop playsinline>
              <source src="https://github.com/ProjFOAM/ProjFOAM.github.io/raw/refs/heads/main/videos/PickTestTube/PickTheFirstTestTube.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="item PickTheSecondTestTube">
          <div class="video-wrapper">
            <video poster="" id="PickTheSecondTestTube" autoplay controls muted loop playsinline>
              <source src="https://github.com/ProjFOAM/ProjFOAM.github.io/raw/refs/heads/main/videos/PickTestTube/PickTheSecondTestTube.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="item PickTheThirdTestTube">
          <div class="video-wrapper">
            <video poster="" id="PickTheThirdTestTube" autoplay controls muted loop playsinline>
              <source src="https://github.com/ProjFOAM/ProjFOAM.github.io/raw/refs/heads/main/videos/PickTestTube/PickTheThirdTestTube.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="item PickTheForthTestTube">
          <div class="video-wrapper">
            <video poster="" id="PickTheForthTestTube" autoplay controls muted loop playsinline>
              <source src="https://github.com/ProjFOAM/ProjFOAM.github.io/raw/refs/heads/main/videos/PickTestTube/PickTheForthTestTube.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="item InsertFirstHole">
          <div class="video-wrapper">
            <video poster="" id="InsertFirstHole" autoplay controls muted loop playsinline>
              <source src="https://github.com/ProjFOAM/ProjFOAM.github.io/raw/refs/heads/main/videos/InsertTestTubeToHole/InsertFirstHole.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="item InsertSecondHole">
          <div class="video-wrapper">
            <video poster="" id="InsertSecondHole" autoplay controls muted loop playsinline>
              <source src="https://github.com/ProjFOAM/ProjFOAM.github.io/raw/refs/heads/main/videos/InsertTestTubeToHole/InsertSecondHole.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="item InsertThirdHole">
          <div class="video-wrapper">
            <video poster="" id="InsertThirdHole" autoplay controls muted loop playsinline>
              <source src="https://github.com/ProjFOAM/ProjFOAM.github.io/raw/refs/heads/main/videos/InsertTestTubeToHole/InsertThirdHole.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="item InsertForthHole">
          <div class="video-wrapper">
            <video poster="" id="InsertForthHole" autoplay controls muted loop playsinline>
              <source src="https://github.com/ProjFOAM/ProjFOAM.github.io/raw/refs/heads/main/videos/InsertTestTubeToHole/InsertForthHole.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="item PutBitterMelonToBowl">
          <div class="video-wrapper">
            <video poster="" id="PutBitterMelonToBowl" autoplay controls muted loop playsinline>
              <source src="https://github.com/ProjFOAM/ProjFOAM.github.io/raw/refs/heads/main/videos/PutStuff2Bowl/PutBitterMelonToBowl.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="item PutEggplantToBowl">
          <div class="video-wrapper">
            <video poster="" id="PutEggplantToBowl" autoplay controls muted loop playsinline>
              <source src="https://github.com/ProjFOAM/ProjFOAM.github.io/raw/refs/heads/main/videos/PutStuff2Bowl/PutEggplantToBowl.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="item PutPeachToGreenBowl">
          <div class="video-wrapper">
            <video poster="" id="PutPeachToGreenBowl" autoplay controls muted loop playsinline>
              <source src="https://github.com/ProjFOAM/ProjFOAM.github.io/raw/refs/heads/main/videos/PutStuff2Bowl/PutPeachToGreenBowl.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="item PutTomatoToGreenBowl">
          <div class="video-wrapper">
            <video poster="" id="PutTomatoToGreenBowl" autoplay controls muted loop playsinline>
              <source src="https://github.com/ProjFOAM/ProjFOAM.github.io/raw/refs/heads/main/videos/PutStuff2Bowl/PutTomatoToGreenBowl.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="item PutBitterMelonToLockerBottomLayer">
          <div class="video-wrapper">
            <video poster="" id="PutBitterMelonToLockerBottomLayer" autoplay controls muted loop playsinline>
              <source src="https://github.com/ProjFOAM/ProjFOAM.github.io/raw/refs/heads/main/videos/PutBitterMelonToLockerLayer/PutBitterMelonToLockerBottomLayer.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="item PutBitterMelonToLockerMiddleLayer">
          <div class="video-wrapper">
            <video poster="" id="PutBitterMelonToLockerMiddleLayer" autoplay controls muted loop playsinline>
              <source src="https://github.com/ProjFOAM/ProjFOAM.github.io/raw/refs/heads/main/videos/PutBitterMelonToLockerLayer/PutBitterMelonToLockerMiddleLayer.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Multi-task imitation learning (MTIL) has shown significant potential in robotic
            manipulation by enabling agents to perform various tasks using a unified policy.
            This simplifies the policy deployment and enhances the agent's adaptability across different contexts.
            However, key challenges remain, such as maintaining action reliability
            (e.g., avoiding abnormal action sequences that deviate from nominal task trajectories),
            distinguishing between similar tasks, and generalizing to unseen scenarios.
            To address these challenges, we introduce the Foresight-Augmented Manipulation Policy (FoAM),
            an innovative MTIL framework.
            FoAM not only learns to mimic expert actions but also predicts the visual outcomes of those actions to enhance decision-making.
            Additionally, it integrates multi-modal goal inputs, such as visual and language prompts,
            overcoming the limitations of single-conditioned policies.
            We evaluated FoAM across over 100 tasks in both simulation and real-world settings,
            demonstrating that it significantly improves IL policy performance, outperforming current state-of-the-art IL baselines by up to 41% in success rate.
            Furthermore, we released a simulation benchmark for robotic manipulation,
            featuring 10 task suites and over 80 challenging tasks designed for multi-task policy training and evaluation.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Fine-tuned Goal Imagination Module</h2>
        <div class="content has-text-justified">
          <p>
            We introduce a goal imagination module into the FoAM framework to improve the agent's autonomy in achieving
            desired goal states. We selected
            <a href="https://www.timothybrooks.com/instruct-pix2pix" target="_blank">InstructPix2Pix</a>
            as our goal imagination module, utilizing
            approximately 20,000 pairs of training data. Of these, 16,000 pairs were derived from the cleaning robot
            expert demonstrations provided by <a href="https://www.timothybrooks.com/instruct-pix2pix" target="_blank">RT-1</a>.
            In this cleaning process, the first and last frames of the
            demonstrations were used as the original and edited images, respectively, with the corresponding task
            name serving as the instruction. Given that many of the demonstration datasets contained perturbations
            in the final frames caused by robot arm movements, we undertook a detailed data cleaning procedure to
            remove noise and ensure the training data quality. Additionally, we incorporated over 4,000 data pairs
            sourced from our own simulation and real-world datasets. We fine-tuned the model for 500 epochs on a
            single NVIDIA H100 GPU, a process that required approximately 3 days. During the image generation stage,
            with the model weights pre-loaded, processing each initial observation of size 480&times;640&times;3
            took about 4 seconds.
            The figures below illustrate the demonstrations of the fine-tuned model in both simulation and
            real-world scenarios. For each demonstration, the image on the left represents the initial visual
            observation, while the image on the right depicts the goal image generated according to the given
            language prompt.
            <br>
              <div style="display: flex; justify-content: center; gap: 20px;">
                <img src="https://github.com/ProjFOAM/ProjFOAM.github.io/raw/refs/heads/main/images/ip2p_1.png" alt="Figure 1" style="width: 70%;">
                <img src="https://github.com/ProjFOAM/ProjFOAM.github.io/raw/refs/heads/main/images/ip2p_2.png" alt="Figure 2" style="width: 70%;">
              </div>

              <div style="display: flex; justify-content: center; gap: 20px; margin-top: 20px;">
                <img src="https://github.com/ProjFOAM/ProjFOAM.github.io/raw/refs/heads/main/images/ip2p_3.png" alt="Figure 3" style="width: 70%;">
                <img src="https://github.com/ProjFOAM/ProjFOAM.github.io/raw/refs/heads/main/images/ip2p_5.png" alt="Figure 5" style="width: 70%;">
              </div>

              <div style="display: flex; justify-content: center; gap: 20px; margin-top: 20px;">
                <img src="https://github.com/ProjFOAM/ProjFOAM.github.io/raw/refs/heads/main/images/ip2p_6.png" alt="Figure 6" style="width: 70%;">
                <img src="https://github.com/ProjFOAM/ProjFOAM.github.io/raw/refs/heads/main/images/ip2p_7.png" alt="Figure 7" style="width: 70%;">
              </div>

              <div style="display: flex; justify-content: center; gap: 20px; margin-top: 20px;">
                <img src="https://github.com/ProjFOAM/ProjFOAM.github.io/raw/refs/heads/main/images/ip2p_8.png" alt="Figure 8" style="width: 70%;">
                <img src="https://github.com/ProjFOAM/ProjFOAM.github.io/raw/refs/heads/main/images/ip2p_9.png" alt="Figure 9" style="width: 70%;">
              </div>
            <br>
            Our experiments showed that FoAM with goal imagination module demonstrated more robust performance than FoAM.
            We attribute this to the deep semantic information retained in the images
            generated by VLM, which helps prevent the model from overfitting when working with
            small datasets. Furthermore, the goal images generated by VLM maintain a consistent
            overall style. This style uniformity ensures that goal images generated at different times
            share similar features, enhancing the robot's ability to adapt to dynamic real-world conditions,
            thereby improving task activation reliability. Additionally, with the introduction of VLM, the agent
            can autonomously and efficiently acquire the goal image, with a 480&times;640 pixel goal image being
            obtained in an average of 4 seconds.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">FoAM Benchmark</h2>
        <div class="content has-text-justified">
          <p>
            We developed a simulated dual-arm robotic system, with each arm possessing 6 DoF
            and a 1-dof parallel-jaw gripper, closely replicating a commonly used UR3e
            robot. A total of 86 simulation tasks (including 4 unseen tasks) were designed,
            encompassing a broad range of practical skills, such as picking, moving, pushing,
            placing, sliding, inserting, opening, closing, and transferring.
            The following video provides an overview of the multi-task scenarios in the FoAM-benchmark.
            <br>
              <div class="container is-max-desktop">
                <!-- FoAMBecnmark overview. -->
                <div class="columns is-centered">
                <!-- External Disturbance Video -->
                <div class="column">
                  <div class="content">
                    <video id="benchmarkoverview" autoplay muted controls loop playsinline height="100%">
                      <source src="https://github.com/ProjFOAM/ProjFOAM.github.io/raw/refs/heads/main/videos/FoAMBenchmark/FoAMBechmarkOverview.mp4" type="video/mp4">
                    </video>
                  </div>
                </div>
              </div>
            <br>
            The tasks in the benchmark are categorized into five categories for performance analysis: pink for dual-arm tasks,
            yellow for cabinet-based tasks, green for block-based tasks, orange for locker-based tasks, and gray for other tasks.
            The FoAM-benchmark offers high-degree-of-freedom simulation suites.
            FoAM-benchmark code, evaluation metrics for each task, and customization tutorials for creating own tasks are available on
            <a href="https://github.com/LitaoLiu01/FoAM-benchmark" target="_blank">FoAM-benchmark</a>.

          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Robustness Analysis</h2>
        <div class="content has-text-justified">
          <p>
            We conducted an in-depth exploration of FoAM, focusing on three key aspects:
            <strong>external disturbance</strong>, <strong>reactiveness</strong>, and <strong>unseen task generalization</strong>.
            <br>
            <br>
            <strong>External Disturbance:</strong> Despite the introduction of additional objects to disrupt the operation process,
            the robot was able to complete the task without signifcant diffculties.
            <br>
            <br>
            <strong>Reactiveness:</strong> During the task execution, we forcibly removed the object from the gripper. In response,
            the robot exhibited the ability to attempt re-grasping the object and ultimately complete the task.
            <br>
            <br>
            <strong>Unseen Task Generalization:</strong> To evaluate FoAM's performance on unseen tasks, we substituted the eggplant with a carambola in the <em>Put Fruits into the Green Bowl</em> real-world scenario.
            FoAM demonstrated the highest success rate compared to other strong baselines.
            <br>
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
  <br>
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <!-- External Disturbance Video -->
      <div class="column">
        <div class="content">
          <h3 class="title is-3">External Disturbance</h3>
          <video id="dollyzoom" autoplay muted controls loop playsinline height="100%">
            <source src="https://github.com/ProjFOAM/ProjFOAM.github.io/raw/refs/heads/main/videos/RobutnessTest/ExternalDisturbance.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <!-- Reactiveness Video -->
      <div class="column">
        <h3 class="title is-3">Reactiveness</h3>
        <div class="columns is-centered">
          <div class="column content">
            <video id="matting-video" autoplay muted loop controls playsinline height="100%">
              <source src="https://github.com/ProjFOAM/ProjFOAM.github.io/raw/refs/heads/main/videos/RobutnessTest/React.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>

    <!-- Unseen Task Generalization Video in a New Row -->
    <div class="columns is-centered">
      <div class="column is-half">
        <div class="content">
          <h3 class="title is-3 has-text-centered">Unseen Task Generalization</h3>
          <video id="unseentask-video" autoplay muted loop controls playsinline width="100%">
            <source src="https://github.com/ProjFOAM/ProjFOAM.github.io/raw/refs/heads/main/videos/PutStuff2Bowl/PutCarambolaToGreenBowl.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>

</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Conclusion and Future Work</h2>
        <div class="content has-text-justified">
          <p>
            In this work, we introduced FoAM, a novel multimodal goal-conditioned policy designed
            to enhance the performance of multi-task policies and address the limitations of single
            goal-conditioned approaches. Inspired by human behavior, FoAM improves agent performance
            by imitating expert actions while simultaneously considering the visual outcomes of those
            actions. In our published FoAM-benchmark and across real-world scenarios, FoAM achieved
            improvements of up to 41\% in success rate compared with previous methods. However, FoAM
            exhibited certain limitations in real-world Scenarios I and II, which involve high
            precision requirements. To address this, we will explore to refine long-horizon tasks by
            generating fine-grained intermediate goal images to serve as guidance. By leveraging
            these intermediate visual states, we seek to reduce cumulative errors during operations
            and improve the agent’s execution accuracy.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p>
                        Website template inspired by <a href="https://github.com/nerfies/nerfies.github.io">Nerfies:
                            Deformable Neural Radiance Fields</a>
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

  <!-- 自动生成目录的 JavaScript -->
  <script>
    document.addEventListener("DOMContentLoaded", function () {
      const tocList = document.getElementById('toc-list'); // 获取目录容器
      const headings = document.querySelectorAll('h2'); // 查找所有 h2 标签

      headings.forEach((heading, index) => {
        // 为每个 h2 设置一个唯一的 id
        const id = `heading-${index}`;
        heading.id = id;

        // 创建目录项
        const li = document.createElement('li');
        const link = document.createElement('a');
        link.href = `#${id}`; // 设置锚点链接
        link.textContent = heading.textContent; // 使用 h2 的文本作为目录项

        li.appendChild(link);
        tocList.appendChild(li); // 添加到目录列表中
      });
    });
  </script>

</body>
</html>
